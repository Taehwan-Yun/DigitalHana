{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14d8c826-7202-455f-9390-8236e4078a38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Packages\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module, Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import argparse\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "05c85390-4ea3-4c81-be66-46dc2bf61308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Datasets\n",
    "a = pickle.load(open('../data/' + 'train.txt', 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1b163a90-52f8-4978-9638-b56edef3b8ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "li = []\n",
    "\n",
    "for i in a[0]:\n",
    "    for j in i:\n",
    "        li.append(j)\n",
    "\n",
    "for i in a[1]:\n",
    "    li.append(i)\n",
    "\n",
    "len(set(li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "814508a3-c5d0-4a0f-a6b8-fec45954baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Graph\n",
    "# Original Data -> Graph Data\n",
    "def build_graph(train_data):\n",
    "    graph = nx.DiGraph()\n",
    "    for seq in train_data:\n",
    "        for i in range(len(seq) - 1):\n",
    "            if graph.get_edge_data(seq[i], seq[i + 1]) is None:\n",
    "                weight = 1\n",
    "            else:\n",
    "                weight = graph.get_edge_data(seq[i], seq[i + 1])['weight'] + 1\n",
    "            graph.add_edge(seq[i], seq[i + 1], weight=weight)\n",
    "    for node in graph.nodes:\n",
    "        sum = 0\n",
    "        for j, i in graph.in_edges(node):\n",
    "            sum += graph.get_edge_data(j, i)['weight']\n",
    "        if sum != 0:\n",
    "            for j, i in graph.in_edges(i):\n",
    "                graph.add_edge(j, i, weight=graph.get_edge_data(j, i)['weight'] / sum)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e94dccb5-6005-477e-8936-bca888a0e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph(Node) Masking\n",
    "def data_masks(all_usr_pois, item_tail):\n",
    "    us_lens = [len(upois) for upois in all_usr_pois]\n",
    "    len_max = max(us_lens)\n",
    "    us_pois = [upois + item_tail * (len_max - le) for upois, le in zip(all_usr_pois, us_lens)]\n",
    "    us_msks = [[1] * le + [0] * (len_max - le) for le in us_lens]\n",
    "    return us_pois, us_msks, len_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e7bcc44d-f9e4-4049-8a6d-398eeaaa0611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Validataion Dataset\n",
    "def split_validation(train_set, valid_portion):\n",
    "    train_set_x, train_set_y = train_set\n",
    "    n_samples = len(train_set_x)\n",
    "    sidx = np.arange(n_samples, dtype='int32')\n",
    "    np.random.shuffle(sidx)\n",
    "    n_train = int(np.round(n_samples * (1. - valid_portion)))\n",
    "    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n",
    "    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n",
    "    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n",
    "    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n",
    "\n",
    "    return (train_set_x, train_set_y), (valid_set_x, valid_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92ad9c02-f921-43a8-b330-a412f71e3d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Class\n",
    "class Data():\n",
    "    def __init__(self, data, shuffle=False, graph=None):\n",
    "        inputs = data[0]\n",
    "        inputs, mask, len_max = data_masks(inputs, [0])\n",
    "        self.inputs = np.asarray(inputs)\n",
    "        self.mask = np.asarray(mask)\n",
    "        self.len_max = len_max\n",
    "        self.targets = np.asarray(data[1])\n",
    "        self.length = len(inputs)\n",
    "        self.shuffle = shuffle\n",
    "        self.graph = graph\n",
    "\n",
    "    def generate_batch(self, batch_size):\n",
    "        if self.shuffle:\n",
    "            shuffled_arg = np.arange(self.length)\n",
    "            np.random.shuffle(shuffled_arg)\n",
    "            self.inputs = self.inputs[shuffled_arg]\n",
    "            self.mask = self.mask[shuffled_arg]\n",
    "            self.targets = self.targets[shuffled_arg]\n",
    "        n_batch = int(self.length / batch_size)\n",
    "        if self.length % batch_size != 0:\n",
    "            n_batch += 1\n",
    "        slices = np.split(np.arange(n_batch * batch_size), n_batch)\n",
    "        slices[-1] = slices[-1][:(self.length - batch_size * (n_batch - 1))]\n",
    "        return slices\n",
    "\n",
    "    def get_slice(self, i):\n",
    "        inputs, mask, targets = self.inputs[i], self.mask[i], self.targets[i]\n",
    "        items, n_node, A, alias_inputs = [], [], [], []\n",
    "        for u_input in inputs:\n",
    "            n_node.append(len(np.unique(u_input)))\n",
    "        max_n_node = np.max(n_node)\n",
    "        for u_input in inputs:\n",
    "            node = np.unique(u_input)\n",
    "            items.append(node.tolist() + (max_n_node - len(node)) * [0])\n",
    "            u_A = np.zeros((max_n_node, max_n_node))\n",
    "            for i in np.arange(len(u_input) - 1):\n",
    "                if u_input[i + 1] == 0:\n",
    "                    break\n",
    "                u = np.where(node == u_input[i])[0][0]\n",
    "                v = np.where(node == u_input[i + 1])[0][0]\n",
    "                u_A[u][v] = 1\n",
    "            u_sum_in = np.sum(u_A, 0)\n",
    "            u_sum_in[np.where(u_sum_in == 0)] = 1\n",
    "            u_A_in = np.divide(u_A, u_sum_in)\n",
    "            u_sum_out = np.sum(u_A, 1)\n",
    "            u_sum_out[np.where(u_sum_out == 0)] = 1\n",
    "            u_A_out = np.divide(u_A.transpose(), u_sum_out)\n",
    "            u_A = np.concatenate([u_A_in, u_A_out]).transpose()\n",
    "            A.append(u_A)\n",
    "            alias_inputs.append([np.where(node == i)[0][0] for i in u_input])\n",
    "        return alias_inputs, A, items, mask, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "223369f0-2791-4b81-b466-73688cc34f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class GNN(Module):\n",
    "    def __init__(self, hidden_size, step=1):\n",
    "        super(GNN, self).__init__()\n",
    "        self.step = step\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = hidden_size * 2\n",
    "        self.gate_size = 3 * hidden_size\n",
    "        self.w_ih = Parameter(torch.Tensor(self.gate_size, self.input_size))\n",
    "        self.w_hh = Parameter(torch.Tensor(self.gate_size, self.hidden_size))\n",
    "        self.b_ih = Parameter(torch.Tensor(self.gate_size))\n",
    "        self.b_hh = Parameter(torch.Tensor(self.gate_size))\n",
    "        self.b_iah = Parameter(torch.Tensor(self.hidden_size))\n",
    "        self.b_oah = Parameter(torch.Tensor(self.hidden_size))\n",
    "\n",
    "        self.linear_edge_in = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
    "        self.linear_edge_out = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
    "        self.linear_edge_f = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
    "\n",
    "    def GNNCell(self, A, hidden):\n",
    "        input_in = torch.matmul(A[:, :, :A.shape[1]], self.linear_edge_in(hidden)) + self.b_iah\n",
    "        input_out = torch.matmul(A[:, :, A.shape[1]: 2 * A.shape[1]], self.linear_edge_out(hidden)) + self.b_oah\n",
    "        inputs = torch.cat([input_in, input_out], 2)\n",
    "        gi = F.linear(inputs, self.w_ih, self.b_ih)\n",
    "        gh = F.linear(hidden, self.w_hh, self.b_hh)\n",
    "        i_r, i_i, i_n = gi.chunk(3, 2)\n",
    "        h_r, h_i, h_n = gh.chunk(3, 2)\n",
    "        resetgate = torch.sigmoid(i_r + h_r)\n",
    "        inputgate = torch.sigmoid(i_i + h_i)\n",
    "        newgate = torch.tanh(i_n + resetgate * h_n)\n",
    "        hy = newgate + inputgate * (hidden - newgate)\n",
    "        return hy\n",
    "\n",
    "    def forward(self, A, hidden):\n",
    "        for i in range(self.step):\n",
    "            hidden = self.GNNCell(A, hidden)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7dff355f-2c01-448e-91b8-4f4c9cd869ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class SessionGraph(Module):\n",
    "    def __init__(self, hiddenSize, batchSize, nonhybrid, step, lr, l2, lr_dc_step, lr_dc, n_node):\n",
    "        super(SessionGraph, self).__init__()\n",
    "        self.hidden_size = hiddenSize\n",
    "        self.n_node = n_node\n",
    "        self.batch_size = batchSize\n",
    "        self.nonhybrid = nonhybrid\n",
    "        self.embedding = nn.Embedding(self.n_node, self.hidden_size)\n",
    "        self.gnn = GNN(self.hidden_size, step=step)\n",
    "        self.linear_one = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
    "        self.linear_two = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
    "        self.linear_three = nn.Linear(self.hidden_size, 1, bias=False)\n",
    "        self.linear_transform = nn.Linear(self.hidden_size * 2, self.hidden_size, bias=True)\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=l2)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=lr_dc_step, gamma=lr_dc)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def compute_scores(self, hidden, mask):\n",
    "        ht = hidden[torch.arange(mask.shape[0]).long(), torch.sum(mask, 1) - 1]  # batch_size x latent_size\n",
    "        q1 = self.linear_one(ht).view(ht.shape[0], 1, ht.shape[1])  # batch_size x 1 x latent_size\n",
    "        q2 = self.linear_two(hidden)  # batch_size x seq_length x latent_size\n",
    "        alpha = self.linear_three(torch.sigmoid(q1 + q2))\n",
    "        a = torch.sum(alpha * hidden * mask.view(mask.shape[0], -1, 1).float(), 1)\n",
    "        if not self.nonhybrid:\n",
    "            a = self.linear_transform(torch.cat([a, ht], 1))\n",
    "        b = self.embedding.weight[1:]  # n_nodes x latent_size\n",
    "        scores = torch.matmul(a, b.transpose(1, 0))\n",
    "        return scores\n",
    "\n",
    "    def forward(self, inputs, A):\n",
    "        hidden = self.embedding(inputs)\n",
    "        hidden = self.gnn(A, hidden)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "721cc682-823e-4a63-8bd4-51a4b74558b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU or CPU\n",
    "\n",
    "def trans_to_cuda(variable):\n",
    "    if torch.cuda.is_available():\n",
    "        return variable.cuda()\n",
    "    else:\n",
    "        return variable\n",
    "    \n",
    "def trans_to_cpu(variable):\n",
    "    if torch.cuda.is_available():\n",
    "        return variable.cpu()\n",
    "    else:\n",
    "        return variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c71d55c4-85d3-4a18-a10e-63170e5f8d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foward Propagation\n",
    "def forward(model, i, data):\n",
    "    alias_inputs, A, items, mask, targets = data.get_slice(i)\n",
    "    alias_inputs = trans_to_cuda(torch.Tensor(alias_inputs).long())\n",
    "    items = trans_to_cuda(torch.Tensor(items).long())\n",
    "    A = trans_to_cuda(torch.Tensor(A).float())\n",
    "    mask = trans_to_cuda(torch.Tensor(mask).long())\n",
    "    hidden = model(items, A)\n",
    "    get = lambda i: hidden[i][alias_inputs[i]]\n",
    "    seq_hidden = torch.stack([get(i) for i in torch.arange(len(alias_inputs)).long()])\n",
    "    return targets, model.compute_scores(seq_hidden, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "4743029f-a3da-480a-8d59-b47a3a27e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & Evaluation Function\n",
    "def train_test(model, train_data, test_data):\n",
    "    model.scheduler.step()\n",
    "    print('start training: ', datetime.datetime.now())\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    slices = train_data.generate_batch(model.batch_size)\n",
    "    for i, j in zip(slices, np.arange(len(slices))):\n",
    "        model.optimizer.zero_grad()\n",
    "        targets, scores = forward(model, i, train_data)\n",
    "        targets = trans_to_cuda(torch.Tensor(targets).long())\n",
    "        loss = model.loss_function(scores, targets - 1)\n",
    "        loss.backward()\n",
    "        model.optimizer.step()\n",
    "        total_loss += loss\n",
    "        if j % int(len(slices) / 5 + 1) == 0:\n",
    "            print('[%d/%d] Loss: %.4f' % (j, len(slices), loss.item()))\n",
    "    print('\\tLoss:\\t%.3f' % total_loss)\n",
    "\n",
    "    print('start predicting: ', datetime.datetime.now())\n",
    "    model.eval()\n",
    "    hit, mrr = [], []\n",
    "    slices = test_data.generate_batch(model.batch_size)\n",
    "    for i in slices:\n",
    "        targets, scores = forward(model, i, test_data)\n",
    "        sub_scores = scores.topk(20)[1]\n",
    "        sub_scores = trans_to_cpu(sub_scores).detach().numpy()\n",
    "        for score, target, mask in zip(sub_scores, targets, test_data.mask):\n",
    "            hit.append(np.isin(target - 1, score))\n",
    "            if len(np.where(score == target - 1)[0]) == 0:\n",
    "                mrr.append(0)\n",
    "            else:\n",
    "                mrr.append(1 / (np.where(score == target - 1)[0][0] + 1))\n",
    "    hit = np.mean(hit) * 100\n",
    "    mrr = np.mean(mrr) * 100\n",
    "    return hit, mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b31fece3-0121-4c2f-aa96-1c19b22d09c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all_train_seq = pickle.load(open('../data' + '/all_train_seq.txt', 'rb'))\n",
    "# g = build_graph(all_train_seq)\n",
    "# nx.draw(g, pos = nx.spring_layout(g), with_labels = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "7022e00c-3ec2-4184-bef7-8f4efd303d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "epoch:  0\n",
      "start training:  2025-01-30 00:36:40.156268\n",
      "[0/99] Loss: 5.9439\n",
      "[20/99] Loss: 5.9168\n",
      "[40/99] Loss: 5.5328\n",
      "[60/99] Loss: 5.6015\n",
      "[80/99] Loss: 5.5697\n",
      "\tLoss:\t565.912\n",
      "start predicting:  2025-01-30 00:36:41.353013\n",
      "Best Result:\n",
      "\tRecall@20:\t31.3230\tMMR@20:\t7.8885\tEpoch:\t0,\t0\n",
      "-------------------------------------------------------\n",
      "epoch:  1\n",
      "start training:  2025-01-30 00:36:41.553610\n",
      "[0/99] Loss: 5.2940\n",
      "[20/99] Loss: 5.2363\n",
      "[40/99] Loss: 5.1562\n",
      "[60/99] Loss: 5.1557\n",
      "[80/99] Loss: 4.1684\n",
      "\tLoss:\t491.347\n",
      "start predicting:  2025-01-30 00:36:42.819247\n",
      "Best Result:\n",
      "\tRecall@20:\t50.9728\tMMR@20:\t18.6047\tEpoch:\t1,\t1\n",
      "-------------------------------------------------------\n",
      "epoch:  2\n",
      "start training:  2025-01-30 00:36:43.031718\n",
      "[0/99] Loss: 4.0136\n",
      "[20/99] Loss: 4.8129\n",
      "[40/99] Loss: 4.2206\n",
      "[60/99] Loss: 3.8404\n",
      "[80/99] Loss: 4.5757\n",
      "\tLoss:\t419.049\n",
      "start predicting:  2025-01-30 00:36:44.182932\n",
      "Best Result:\n",
      "\tRecall@20:\t52.7237\tMMR@20:\t20.0630\tEpoch:\t2,\t2\n",
      "-------------------------------------------------------\n",
      "epoch:  3\n",
      "start training:  2025-01-30 00:36:44.446750\n",
      "[0/99] Loss: 3.8224\n",
      "[20/99] Loss: 4.4729\n",
      "[40/99] Loss: 4.1760\n",
      "[60/99] Loss: 4.4127\n",
      "[80/99] Loss: 3.9323\n",
      "\tLoss:\t406.140\n",
      "start predicting:  2025-01-30 00:36:45.465531\n",
      "Best Result:\n",
      "\tRecall@20:\t54.2802\tMMR@20:\t20.6574\tEpoch:\t3,\t3\n",
      "-------------------------------------------------------\n",
      "epoch:  4\n",
      "start training:  2025-01-30 00:36:45.662302\n",
      "[0/99] Loss: 3.7055\n",
      "[20/99] Loss: 3.7046\n",
      "[40/99] Loss: 3.8838\n",
      "[60/99] Loss: 3.5900\n",
      "[80/99] Loss: 3.8654\n",
      "\tLoss:\t395.533\n",
      "start predicting:  2025-01-30 00:36:46.737387\n",
      "Best Result:\n",
      "\tRecall@20:\t55.2529\tMMR@20:\t20.7620\tEpoch:\t4,\t4\n",
      "-------------------------------------------------------\n",
      "epoch:  5\n",
      "start training:  2025-01-30 00:36:46.990571\n",
      "[0/99] Loss: 3.9759\n",
      "[20/99] Loss: 4.5269\n",
      "[40/99] Loss: 4.2172\n",
      "[60/99] Loss: 3.8996\n",
      "[80/99] Loss: 3.7127\n",
      "\tLoss:\t387.494\n",
      "start predicting:  2025-01-30 00:36:48.289432\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t20.9100\tEpoch:\t5,\t5\n",
      "-------------------------------------------------------\n",
      "epoch:  6\n",
      "start training:  2025-01-30 00:36:48.549396\n",
      "[0/99] Loss: 4.2547\n",
      "[20/99] Loss: 4.0211\n",
      "[40/99] Loss: 4.0947\n",
      "[60/99] Loss: 4.5231\n",
      "[80/99] Loss: 4.7324\n",
      "\tLoss:\t386.421\n",
      "start predicting:  2025-01-30 00:36:49.763083\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t20.9917\tEpoch:\t5,\t6\n",
      "-------------------------------------------------------\n",
      "epoch:  7\n",
      "start training:  2025-01-30 00:36:49.996410\n",
      "[0/99] Loss: 3.6719\n",
      "[20/99] Loss: 4.2749\n",
      "[40/99] Loss: 4.2530\n",
      "[60/99] Loss: 3.6474\n",
      "[80/99] Loss: 3.9897\n",
      "\tLoss:\t385.632\n",
      "start predicting:  2025-01-30 00:36:51.286564\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0483\tEpoch:\t7,\t7\n",
      "-------------------------------------------------------\n",
      "epoch:  8\n",
      "start training:  2025-01-30 00:36:51.572316\n",
      "[0/99] Loss: 3.0959\n",
      "[20/99] Loss: 3.8929\n",
      "[40/99] Loss: 4.0069\n",
      "[60/99] Loss: 3.9999\n",
      "[80/99] Loss: 3.9385\n",
      "\tLoss:\t385.204\n",
      "start predicting:  2025-01-30 00:36:52.843732\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0572\tEpoch:\t8,\t8\n",
      "-------------------------------------------------------\n",
      "epoch:  9\n",
      "start training:  2025-01-30 00:36:53.100015\n",
      "[0/99] Loss: 4.1095\n",
      "[20/99] Loss: 4.1197\n",
      "[40/99] Loss: 4.5282\n",
      "[60/99] Loss: 4.0164\n",
      "[80/99] Loss: 3.3111\n",
      "\tLoss:\t384.632\n",
      "start predicting:  2025-01-30 00:36:54.295210\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0572\tEpoch:\t9,\t8\n",
      "-------------------------------------------------------\n",
      "epoch:  10\n",
      "start training:  2025-01-30 00:36:54.541860\n",
      "[0/99] Loss: 4.1425\n",
      "[20/99] Loss: 4.4745\n",
      "[40/99] Loss: 4.1175\n",
      "[60/99] Loss: 3.0906\n",
      "[80/99] Loss: 4.8758\n",
      "\tLoss:\t384.797\n",
      "start predicting:  2025-01-30 00:36:55.804911\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0580\tEpoch:\t10,\t10\n",
      "-------------------------------------------------------\n",
      "epoch:  11\n",
      "start training:  2025-01-30 00:36:56.029540\n",
      "[0/99] Loss: 4.4497\n",
      "[20/99] Loss: 4.0926\n",
      "[40/99] Loss: 3.3150\n",
      "[60/99] Loss: 4.3683\n",
      "[80/99] Loss: 3.7215\n",
      "\tLoss:\t384.523\n",
      "start predicting:  2025-01-30 00:36:57.246457\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0580\tEpoch:\t11,\t11\n",
      "-------------------------------------------------------\n",
      "epoch:  12\n",
      "start training:  2025-01-30 00:36:57.483629\n",
      "[0/99] Loss: 3.4406\n",
      "[20/99] Loss: 4.1254\n",
      "[40/99] Loss: 3.5004\n",
      "[60/99] Loss: 4.1898\n",
      "[80/99] Loss: 4.1352\n",
      "\tLoss:\t384.968\n",
      "start predicting:  2025-01-30 00:36:58.711320\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0580\tEpoch:\t12,\t11\n",
      "-------------------------------------------------------\n",
      "epoch:  13\n",
      "start training:  2025-01-30 00:36:58.963916\n",
      "[0/99] Loss: 4.2168\n",
      "[20/99] Loss: 3.9670\n",
      "[40/99] Loss: 3.8806\n",
      "[60/99] Loss: 3.9632\n",
      "[80/99] Loss: 4.5806\n",
      "\tLoss:\t385.021\n",
      "start predicting:  2025-01-30 00:37:00.310671\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0580\tEpoch:\t13,\t11\n",
      "-------------------------------------------------------\n",
      "epoch:  14\n",
      "start training:  2025-01-30 00:37:00.563036\n",
      "[0/99] Loss: 4.0833\n",
      "[20/99] Loss: 4.1706\n",
      "[40/99] Loss: 4.2844\n",
      "[60/99] Loss: 3.6037\n",
      "[80/99] Loss: 3.9777\n",
      "\tLoss:\t384.743\n",
      "start predicting:  2025-01-30 00:37:01.767772\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0580\tEpoch:\t14,\t11\n",
      "-------------------------------------------------------\n",
      "epoch:  15\n",
      "start training:  2025-01-30 00:37:01.991897\n",
      "[0/99] Loss: 3.6515\n",
      "[20/99] Loss: 3.6540\n",
      "[40/99] Loss: 3.8869\n",
      "[60/99] Loss: 3.1513\n",
      "[80/99] Loss: 3.2755\n",
      "\tLoss:\t384.546\n",
      "start predicting:  2025-01-30 00:37:03.304942\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0580\tEpoch:\t15,\t11\n",
      "-------------------------------------------------------\n",
      "epoch:  16\n",
      "start training:  2025-01-30 00:37:03.554647\n",
      "[0/99] Loss: 4.4360\n",
      "[20/99] Loss: 4.5159\n",
      "[40/99] Loss: 3.4439\n",
      "[60/99] Loss: 3.7818\n",
      "[80/99] Loss: 4.1924\n",
      "\tLoss:\t384.526\n",
      "start predicting:  2025-01-30 00:37:04.725824\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0580\tEpoch:\t16,\t11\n",
      "-------------------------------------------------------\n",
      "epoch:  17\n",
      "start training:  2025-01-30 00:37:04.952660\n",
      "[0/99] Loss: 4.0484\n",
      "[20/99] Loss: 4.3725\n",
      "[40/99] Loss: 4.9217\n",
      "[60/99] Loss: 3.9545\n",
      "[80/99] Loss: 3.0712\n",
      "\tLoss:\t384.832\n",
      "start predicting:  2025-01-30 00:37:06.327480\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0580\tEpoch:\t17,\t11\n",
      "-------------------------------------------------------\n",
      "epoch:  18\n",
      "start training:  2025-01-30 00:37:06.534512\n",
      "[0/99] Loss: 3.5936\n",
      "[20/99] Loss: 4.4168\n",
      "[40/99] Loss: 3.7454\n",
      "[60/99] Loss: 4.1655\n",
      "[80/99] Loss: 4.2481\n",
      "\tLoss:\t384.545\n",
      "start predicting:  2025-01-30 00:37:07.879986\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0580\tEpoch:\t18,\t11\n",
      "-------------------------------------------------------\n",
      "epoch:  19\n",
      "start training:  2025-01-30 00:37:08.085321\n",
      "[0/99] Loss: 3.9225\n",
      "[20/99] Loss: 3.6869\n",
      "[40/99] Loss: 3.1784\n",
      "[60/99] Loss: 4.2507\n",
      "[80/99] Loss: 3.2965\n",
      "\tLoss:\t384.899\n",
      "start predicting:  2025-01-30 00:37:09.263225\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0580\tEpoch:\t19,\t11\n",
      "-------------------------------------------------------\n",
      "epoch:  20\n",
      "start training:  2025-01-30 00:37:09.501013\n",
      "[0/99] Loss: 3.7108\n",
      "[20/99] Loss: 3.5683\n",
      "[40/99] Loss: 3.8329\n",
      "[60/99] Loss: 3.8581\n",
      "[80/99] Loss: 3.8849\n",
      "\tLoss:\t384.169\n",
      "start predicting:  2025-01-30 00:37:10.677617\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0580\tEpoch:\t20,\t11\n",
      "-------------------------------------------------------\n",
      "epoch:  21\n",
      "start training:  2025-01-30 00:37:10.912583\n",
      "[0/99] Loss: 3.5191\n",
      "[20/99] Loss: 4.4255\n",
      "[40/99] Loss: 3.9979\n",
      "[60/99] Loss: 4.1186\n",
      "[80/99] Loss: 3.3767\n",
      "\tLoss:\t384.704\n",
      "start predicting:  2025-01-30 00:37:12.060995\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0580\tEpoch:\t21,\t11\n",
      "-------------------------------------------------------\n",
      "epoch:  22\n",
      "start training:  2025-01-30 00:37:12.288379\n",
      "[0/99] Loss: 3.5808\n",
      "[20/99] Loss: 3.3104\n",
      "[40/99] Loss: 4.2086\n",
      "[60/99] Loss: 3.3105\n",
      "[80/99] Loss: 3.8085\n",
      "\tLoss:\t384.643\n",
      "start predicting:  2025-01-30 00:37:13.485143\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0580\tEpoch:\t22,\t11\n",
      "-------------------------------------------------------\n",
      "epoch:  23\n",
      "start training:  2025-01-30 00:37:13.703837\n",
      "[0/99] Loss: 3.6706\n",
      "[20/99] Loss: 4.4433\n",
      "[40/99] Loss: 4.7338\n",
      "[60/99] Loss: 4.9157\n",
      "[80/99] Loss: 3.8262\n",
      "\tLoss:\t384.492\n",
      "start predicting:  2025-01-30 00:37:14.953593\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0580\tEpoch:\t23,\t11\n",
      "-------------------------------------------------------\n",
      "epoch:  24\n",
      "start training:  2025-01-30 00:37:15.152221\n",
      "[0/99] Loss: 4.2790\n",
      "[20/99] Loss: 3.7184\n",
      "[40/99] Loss: 4.2486\n",
      "[60/99] Loss: 3.8948\n",
      "[80/99] Loss: 3.4158\n",
      "\tLoss:\t384.649\n",
      "start predicting:  2025-01-30 00:37:16.363292\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0580\tEpoch:\t24,\t11\n",
      "-------------------------------------------------------\n",
      "epoch:  25\n",
      "start training:  2025-01-30 00:37:16.586726\n",
      "[0/99] Loss: 4.2161\n",
      "[20/99] Loss: 3.6100\n",
      "[40/99] Loss: 3.7014\n",
      "[60/99] Loss: 4.4310\n",
      "[80/99] Loss: 3.4678\n",
      "\tLoss:\t384.456\n",
      "start predicting:  2025-01-30 00:37:17.810235\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0580\tEpoch:\t25,\t11\n",
      "-------------------------------------------------------\n",
      "epoch:  26\n",
      "start training:  2025-01-30 00:37:18.066902\n",
      "[0/99] Loss: 4.2756\n",
      "[20/99] Loss: 3.3463\n",
      "[40/99] Loss: 3.5839\n",
      "[60/99] Loss: 4.0741\n",
      "[80/99] Loss: 3.1687\n",
      "\tLoss:\t384.732\n",
      "start predicting:  2025-01-30 00:37:19.334776\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0580\tEpoch:\t26,\t11\n",
      "-------------------------------------------------------\n",
      "epoch:  27\n",
      "start training:  2025-01-30 00:37:19.598607\n",
      "[0/99] Loss: 4.3163\n",
      "[20/99] Loss: 3.3768\n",
      "[40/99] Loss: 3.3579\n",
      "[60/99] Loss: 4.4671\n",
      "[80/99] Loss: 3.5333\n",
      "\tLoss:\t384.755\n",
      "start predicting:  2025-01-30 00:37:20.975755\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0580\tEpoch:\t27,\t11\n",
      "-------------------------------------------------------\n",
      "epoch:  28\n",
      "start training:  2025-01-30 00:37:21.240946\n",
      "[0/99] Loss: 3.6983\n",
      "[20/99] Loss: 4.0941\n",
      "[40/99] Loss: 4.2220\n",
      "[60/99] Loss: 3.6213\n",
      "[80/99] Loss: 3.8764\n",
      "\tLoss:\t384.262\n",
      "start predicting:  2025-01-30 00:37:22.630304\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0580\tEpoch:\t28,\t11\n",
      "-------------------------------------------------------\n",
      "epoch:  29\n",
      "start training:  2025-01-30 00:37:22.887338\n",
      "[0/99] Loss: 3.5244\n",
      "[20/99] Loss: 4.0043\n",
      "[40/99] Loss: 3.2879\n",
      "[60/99] Loss: 3.8320\n",
      "[80/99] Loss: 4.0112\n",
      "\tLoss:\t384.307\n",
      "start predicting:  2025-01-30 00:37:24.026599\n",
      "Best Result:\n",
      "\tRecall@20:\t55.4475\tMMR@20:\t21.0580\tEpoch:\t29,\t11\n",
      "-------------------------------------------------------\n",
      "Run time: 44.103490 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batchSize = 16\n",
    "hiddenSize = 80\n",
    "epoch_max = 30\n",
    "lr = 0.001\n",
    "lr_dc = 0.1\n",
    "lr_dc_step = 3\n",
    "l2 = 1e-5\n",
    "step = 1\n",
    "patience = 10\n",
    "nonhybrid = True\n",
    "\n",
    "\n",
    "def main():\n",
    "    train_data = pickle.load(open('../data/' + 'train.txt', 'rb'))    \n",
    "    test_data = pickle.load(open('../data/' + '/test.txt', 'rb'))\n",
    "    train_data = Data(train_data, shuffle=True)\n",
    "    test_data = Data(test_data, shuffle=False)\n",
    "    n_node = 381\n",
    "\n",
    "    model = trans_to_cuda(SessionGraph(hiddenSize, batchSize, nonhybrid, step, lr, l2, lr_dc_step, lr_dc, n_node))\n",
    "\n",
    "    start = time.time()\n",
    "    best_result = [0, 0]\n",
    "    best_epoch = [0, 0]\n",
    "    bad_counter = 0\n",
    "    for epoch in range(epoch_max):\n",
    "        print('-------------------------------------------------------')\n",
    "        print('epoch: ', epoch)\n",
    "        hit, mrr = train_test(model, train_data, test_data)\n",
    "        flag = 0\n",
    "        if hit >= best_result[0]:\n",
    "            best_result[0] = hit\n",
    "            best_epoch[0] = epoch\n",
    "            flag = 1\n",
    "        if mrr >= best_result[1]:\n",
    "            best_result[1] = mrr\n",
    "            best_epoch[1] = epoch\n",
    "            flag = 1\n",
    "        print('Best Result:')\n",
    "        print('\\tRecall@20:\\t%.4f\\tMMR@20:\\t%.4f\\tEpoch:\\t%d,\\t%d'% (best_result[0], best_result[1], best_epoch[0], best_epoch[1]))\n",
    "        bad_counter += 1 - flag\n",
    "        if bad_counter >= patience:\n",
    "            break\n",
    "    print('-------------------------------------------------------')\n",
    "    end = time.time()\n",
    "    print(\"Run time: %f s\" % (end - start))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726fecc9-7b6c-41e6-92dd-7e23f53c71e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
